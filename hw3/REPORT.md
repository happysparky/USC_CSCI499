python ./train.py --in_data_fn="./lang_to_sem_data.json" 

Preprocessing:
- encoder inputs don't have start token/end token (BOS and EOS)
- encoder inputs have padding



https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb

https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

https://www.guru99.com/seq2seq-model.html